# Mish_Activation

### Overview
Mish is a novel smooth, self-regularized and non-monotonic activation function.
Mish function has outperformed popularly used activation functions like ReLU and Swish in over 70 different criteria of problems on challenging datasets like CIFAR-10, CIFAR-100, CalTech-256, ASL etc



### Results

![Alt text](https://github.com/gurucharanmk/Mish_Activation/blob/main/images/results.png  )

## License
This project is licensed under the [MIT License](https://github.com/gurucharanmk/Mish_Activation/blob/main/LICENSE)
